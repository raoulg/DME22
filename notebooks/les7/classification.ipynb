{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basic workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data\n",
    "\n",
    "Let's load the MNIST dataset. This is a dataset of handwritten numbers. You will encounter it in a lot of machine learning tutorials.\n",
    "\n",
    "If you want to run this notebook, you will need the torch extension with mads-datasets.\n",
    "You can update it with:\n",
    "```bash\n",
    "pdm add \"mads-datasets[torch]\"\n",
    "```\n",
    "\n",
    "Because you should already have your pyproject.toml file updated by me if you read this, you can just run:\n",
    "```bash\n",
    "pdm install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "fashiondataset = DatasetFactoryProvider.create_factory(\n",
    "    DatasetType.FASHION\n",
    ")\n",
    "\n",
    "data = fashiondataset.create_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data['train']\n",
    "test = data['valid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape is (60000, 28,28) for the trainset. This means: we have 60000 cases, and every case is a 28x28 matrix. We can visualize a single instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 25 #let's have a look at case 25. You can change this to have a look at others\n",
    "digit, y = train[idx]\n",
    "img = digit.squeeze().numpy()\n",
    "plt.imshow(img, cmap='binary')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with trying to predict the cases with number 3 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.labels.numpy()\n",
    "X_train = train.data.numpy()\n",
    "y_test = valid.labels.numpy()\n",
    "X_test = valid.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_single, y_test_single = (y_train == 3, y_test == 3)\n",
    "\n",
    "np.mean(y_train_single) , np.mean(y_test_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how balanced the dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train, columns = ['train']).\\\n",
    "    groupby('train').\\\n",
    "    size().\\\n",
    "    plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 10% of the dataset is a three. This is what you would expect for an evenly distributed set, which the barplot confirms. Now lets reshape the 28x28 matrices to a vector of 28x28=784 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape the data, because our model can't handle 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the -1 tells reshape: reshape to a matrix where the amount that should be at -1 is deducted from the other amounts.\n",
    "# because the first number is 60.000, reshape will make sure the second value is 784, because that is the only way\n",
    "# to make a matrix with 60.000 rows, in this case.\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are actually doing, is reshaping the grid into one long vector. While that might be a weird representation for an image, a classifier works suprisingly well.\n",
    "\n",
    "Can you understand, what the classifier is doing with this representation? Could you explain in normal language what the strategy of this approach is? The logic of why this works? What would be a downside of this approach?\n",
    "\n",
    "The data ranges from 0 to 255, which is normal for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(X_train[0]), max(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's scale the data to make things a bit easier for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Fitting a model\n",
    "\n",
    "The basic drill is:\n",
    "\n",
    "1. make a train-test split\n",
    "2. Explore the data, preprocess where needed\n",
    "2. select and import a model, set some hyperparameters\n",
    "3. fit the model\n",
    "4. evaluate the model\n",
    "\n",
    "Now, let's see that in code. The most basic syntax is:\n",
    "\n",
    "``` \n",
    "from sklearn import model # import the model\n",
    "clf = model(parameters) # set parameters\n",
    "clf.fit(train_X, train_y) # fit on the data\n",
    "```\n",
    "\n",
    "And we can predict:\n",
    "\n",
    "`yhat = clf.predict(test_X, test_y)`\n",
    "\n",
    "and calculate a score with the metric we pick.\n",
    "\n",
    "Let's try this for a SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier # import the sgd classifier\n",
    "sgd = SGDClassifier(max_iter=10) # we change the max amount of iterations for speedup\n",
    "sgd.fit(X_train, y_train_single) # fit the model\n",
    "yhat = sgd.predict(X_test)  # predict\n",
    "accuracy_score(y_test_single, yhat) # and score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the simples way to select, fit and predict with a model. Sklearn handles everything about the model. We can tune some hyperparameters, but for now we just used the defaults, except for the `max_iter`. This reduces the amount of iterations from the default of 1000 to just 10, because we're just testing here and this speeds things up for testing. Nevertheless, the performance seems to be pretty good (however, there is a catch we will look at in a few moments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Let's visualize what the model is doing, in terms of weight. Can you explain what the model is doing, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sgd.coef_.reshape(28, 28)\n",
    "sns.heatmap(weights, center = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use cross validation to test the performance. Here, we can specify splits of the data. We make 5 different splits, and calculate the average performance. This helps us to reduce the impact of lucky splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd, X_test, y_test_single, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great. But don't cheer to fast... This high percentage is due to the unbalanced dataset. \n",
    "Let's see how a dummy classifier performs, that just picks the most frequent occurence (in our case: 90% is NOT a three, so the dummy will predict that everything is NOT a three.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train_single)\n",
    "cross_val_score(dummy_majority, X_test, y_test_single, cv = 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ai... That are pretty high scores too. Maybe we didn't do as well as simply looking at the accuracy seemed to promise. \n",
    "\n",
    "This should be a lesson about the problems you could encounter when trying to assess performance on an unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull for plotting heatmaps of a confusion matrix\n",
    "def cfm_heatmap(cfm, figsize = (8,8), scale = None, vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    figsize: tuple, default (8,8)\n",
    "    scale: string. The direction over which the numbers are scaled. Either None, 'total', 'rowwise' or 'colwise'\n",
    "    \"\"\"\n",
    "\n",
    "    if (scale == 'total'):\n",
    "        cfm_norm = cfm / np.sum(cfm)\n",
    "    elif (scale == 'rowwise'):\n",
    "        cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "    elif (scale == 'colwise'):\n",
    "        cfm_norm = cfm / np.sum(cfm, axis=0, keepdims=True)\n",
    "    else:\n",
    "        cfm_norm = cfm\n",
    "    plt.figure(figsize=figsize)\n",
    "    plot = sns.heatmap(cfm_norm, annot = cfm_norm, vmin=vmin, vmax=vmax)\n",
    "    plot.set(xlabel = 'Predicted', ylabel = 'Target')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make a confusion matrix. Now it is much clearer what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "yhat_dummy = dummy_majority.predict(X_test)\n",
    "cfm = confusion_matrix(y_test_single, yhat_dummy)\n",
    "cfm_heatmap(cfm, scale = 'rowwise')\n",
    "f1_score(y_test_single, yhat_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is going on? Well, we see that in the column predicted, everything is predicted as a 0. So this means that of the targets with label 0 (not three), we predicted 90% accurate as a 0. But for target 1 (in our case, the number three), we also predicted everything as 'not three'. A nice way to express this is with the f1-score.\n",
    "\n",
    "**Precision**: how many of the samples *predicted* as positive are actually positive\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Recall**: how many of *actual* positive samples are indeed predicted as positive\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**F-score**: the harmonic mean of precision and recall\n",
    "\n",
    "$$ F = 2 * \\frac{precision * recall}{precision + recall} $$\n",
    "\n",
    "If we look at the f1-score, it is actually zero. So, let's make a confusion matrix of the SGD classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = sgd.predict(X_test)\n",
    "cfm = confusion_matrix(y_test_single, y_test_hat)\n",
    "cfm_heatmap(cfm, scale = 'rowwise')\n",
    "f1_score(y_test_single, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. This should also make clear, how you can be deceived with a simple accuracy measure, but you can see a difference in performance if you look at the confusion matrix.\n",
    "\n",
    " We normalized rowwise, wich means that the rows (the actual label) sum up to 1. We see that we predicted 82 percent of the actual threes indeed as a three, making an error in 18% of the cases. We also mistook 1.8% of the non-threes for a three. So let's look at what's going on internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_decision = cross_val_predict(sgd, X_train, y_train_single, cv = 3, n_jobs = 4, method = 'decision_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first few values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = zip(y_decision[5:10], y_train_single[5:10])\n",
    "pd.DataFrame(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably figure out what is going on. Low values mean 'not a three', high values mean 'a three'. By using these decision values, we can change the behavior of the classifier to be more strict, or more loose when it comes deciding if something is a three, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_train_single, y_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'precision':precision[:-1],'recall': recall[:-1], 'thresholds':thresholds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = 'thresholds', y='precision', label = 'precision', data = data)\n",
    "sns.lineplot(x = 'thresholds', y='recall', label = 'recall', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have figured out by now, this plot shows that you can achieve any precision you want! The only problem is that your recall will drop, and vice versa... In some cases you could want to tune this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_single, y_decision)\n",
    "data = pd.DataFrame({'fpr' : fpr, 'tpr':tpr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "plot = sns.lineplot(x = 'fpr', y = 'tpr', data=data)\n",
    "plot.set(xlabel = 'FPR', ylabel = 'TPR')\n",
    "plt.plot([0,1], [0,1], 'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another visualization that is often used, is a ROC-curve. You plot the False Positive Rate against the True Positive Rate. The diagonal line is what you expect from coincence, so you should get away from that.\n",
    "\n",
    "The steep rise means: even though the False Positive Rate is very low, you identify already about 60-80% of the True Positive cases. That is nice!\n",
    "\n",
    "If you also want to get those last difficult cases, you will have to accept that your False Positive rate will also grow, meaning that you will make more mistakes in giving something a label while you should not have done so.\n",
    "\n",
    "#  Multiclass prediction\n",
    "Now let's move on to a more complex case, where we actually want to predict every number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing a simple fit, we can use cross validation. Internally, this splits the dataset in equal parts, fits on one part, predicts on another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sgd = SGDClassifier(random_state=5, max_iter=5, n_jobs=4)\n",
    "yhat = cross_val_predict(sgd, X_train, y_train, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = confusion_matrix(y_train, yhat)\n",
    "cfm_heatmap(cfm, figsize=(12,12), scale='rowwise', vmax= 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem like a lot of information to take in. But, on the other hand, let's not forget that we have 10 cases to predict. This means that we have 10 cases, and every case might get one out of 10 labels. This gives us 100 cases in total. Considering that, the heatmap is a nice way to quickly spot the problems.\n",
    "\n",
    "Again, we normalised over the rows. We see that what is actually a three is often mistaken for a five. The same goes the other way around. Also the seven is often mistaken for a nine.\n",
    "\n",
    "# Scanning models at scale\n",
    "So, due to the [\"no free lunch theorem\"](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization) we might have an intuition about a best model, but there is no best by default and often we will need to simple test and compare. Let's try to scale this up.\n",
    "\n",
    "## create synthetic data\n",
    "Let's create some data and explore it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples = 500, noise = 0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X_train)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data, x=0, y=1, hue=y_train, palette='Set1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick some models\n",
    "Looking around in the sklearn documentation about classifiers, we can pick some classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single classifier, the process would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "result = cross_val_score(svc, X_test, y_test, cv = 5, scoring='f1_macro')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, sure, we could repeat that by copy-pasting these lines. But why copy paste if we can program the repetitive part as a for-loop?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 5\n",
    "classifiers = [\n",
    "    ('svc-linear', SVC(kernel='linear')),\n",
    "    ('svc-kernel', SVC()),\n",
    "    ('random-forest', RandomForestClassifier()),\n",
    "    ('naive bayes', GaussianNB()),\n",
    "    ('gaussian', GaussianProcessClassifier()),\n",
    "    ('kNN', KNeighborsClassifier(3)),\n",
    "    ('decision tree', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "for i, (name, clf) in enumerate(classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    result = cross_val_score(clf, X_test, y_test, cv = cv, scoring='f1_macro')\n",
    "\n",
    "    mu = np.mean(result)\n",
    "    stderr = np.std(result)/np.sqrt(cv)\n",
    "\n",
    "    plt.scatter(i, mu, label=name)\n",
    "    plt.errorbar(i, mu, yerr=stderr)\n",
    "    plt.legend(loc=3)\n",
    "\n",
    "plt.xticks(np.arange(len(classifiers)), [name[0] for name in classifiers], rotation=45);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good. We get a nice overview of performance, even without tweaking the models by changing the hyperparameters.\n",
    "\n",
    "\n",
    "In addition to this, let's set up a contour plot to check the insides of the model (cf., how the model decides on the class of a point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_countour(X_train, y_train, model, granularity=0.1, grid_side=0.5, palette='Set1', ax=None):\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "\n",
    "    # first, we get the min-max range over which we want to plot\n",
    "    # this is the area for which we want to know the behavior of the model\n",
    "    # we add some extra space with grid_side to the feature space.\n",
    "    x0_min, x0_max = X_train.iloc[:,0].min() -grid_side, X_train.iloc[:,0].max() +grid_side\n",
    "    x1_min, x1_max = X_train.iloc[:,1].min() -grid_side, X_train.iloc[:,1].max() +grid_side\n",
    "\n",
    "    # we make a grid of coordinates\n",
    "    xx, yy = np.meshgrid(np.arange(x0_min, x0_max, granularity),\n",
    "                         np.arange(x1_min, x1_max, granularity))\n",
    "    # and combine the grid into a new dataset.\n",
    "    # this new dataset covers (with some granularity) every point of the original dataset\n",
    "    # this newx is equal to the featurespace we want to examine.\n",
    "    newx = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    # we make a prediction with the new dataset. This will show us predictions over the complete featurespace.\n",
    "    yhat = model.predict(newx)\n",
    "\n",
    "    # and reshape the prediction, such that it will match our gridsize\n",
    "    z = yhat.reshape(xx.shape)\n",
    "    cm = sns.color_palette(palette, as_cmap=True)\n",
    "    if ax is None:\n",
    "        # in the case we want to make a single plot\n",
    "        plt.contourf(xx, yy, z, cmap=cm, alpha = 0.5)\n",
    "    else:\n",
    "        # in the case we have subplots and have our own axes to plot on\n",
    "        ax.contourf(xx, yy, z, cmap=cm, alpha = 0.5)\n",
    "\n",
    "    x1, x2 = X_train.iloc[:,0], X_train.iloc[:,1]\n",
    "    sns.scatterplot(x=x1, y=x2, hue=y_train, palette=palette, ax=ax,style=y_train, alpha=0.5, markers={0 : \"s\", 1:\"o\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works for a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countour(X_train, y_train, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, let's scale that up as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 4, figsize=(16,12))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, (name, clf) in enumerate(classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    result = cross_val_score(clf, X_test, y_test, cv = cv, scoring='f1_macro')\n",
    "    plot_countour(X_train, y_train, clf, ax=axs[i], palette=\"Set1\")\n",
    "    axs[i].set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can only use this conveniently for 2D data (because, well, how would you want to plot data that has 8 dimensions? or 30?)\n",
    "\n",
    "Another way to check performance is using precision-recall and roc curves. However, we showed earlier, how to do this for a model with a decision function. But not every model has one, as some models work with probabilities. Those will have a `predict_proba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, clf in classifiers:\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        print(\"decision_function : {}\".format(name))\n",
    "    if hasattr(clf,\"predict_proba\"):\n",
    "        print(\"predict_proba     : {}\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use one of the probability models, we have to make some small modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpc = GaussianProcessClassifier()\n",
    "gpc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = gpc.predict_proba(X_train)\n",
    "proba[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the probabilities are two values. You actually need just one of them (because the other is 1-p). So, we will have to figure out, what labels corresponds to which probability. In this case, a label of value 1 will correspond with a high value in column indexed 1.\n",
    "\n",
    "To make the prediction generalize better, let's use `cross_val_predict`. Note how we need to change the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_decision = cross_val_predict(gpc, X_train, y_train, cv = 3, n_jobs = 4, method = 'predict_proba')\n",
    "y_decision[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_train, y_decision[:,1])\n",
    "data = pd.DataFrame({'precision':precision[:-1],'recall': recall[:-1], 'thresholds':thresholds})\n",
    "sns.lineplot(x = 'thresholds', y='precision', label = 'precision', data = data)\n",
    "sns.lineplot(x = 'thresholds', y='recall', label = 'recall', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train, y_decision[:,1])\n",
    "data = pd.DataFrame({'fpr' : fpr, 'tpr':tpr})\n",
    "plot = sns.lineplot(x = 'fpr', y = 'tpr', data=data)\n",
    "plot.set(xlabel = 'FPR', ylabel = 'TPR')\n",
    "plt.plot([0,1], [0,1], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ca2ed108e0829ad954ac36f354f1cc4b518ca95c97ddb3ce5ba4e3a95bf1dea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
