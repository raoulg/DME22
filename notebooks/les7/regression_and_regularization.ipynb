{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create 100 observations\n",
    "n = 100\n",
    "\n",
    "# this way, our X will go over the range [-3, 3]\n",
    "x = 6 * np.random.rand(n) - 3\n",
    "# let's use a quadratic equation\n",
    "a = 1.5\n",
    "b= 0.7\n",
    "noise = np.random.normal(size=n)\n",
    "\n",
    "y = a * x + b + noise\n",
    "data = {\"x\": x, \"y\": y}\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Note how the model expects a matrix. We need to reshape the data from just 100 numbers to a (100,1) matrix to feed it to the model.\n",
    "\n",
    "Also note how we don't use a train-test split. You should do so if you are testing a model on actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "X = x.reshape(-1, 1)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# make evenly spaced values over the input range\n",
    "newx = np.linspace(min(X), max(X), 100)\n",
    "# and predict with those\n",
    "yhat = linreg.predict(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the performance in different ways. Visual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label = 'data')\n",
    "plt.plot(newx, yhat, label = 'model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a $R^2$ score, ranging from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or with a rmse, which only makes sense when comparing it to another model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(yhat, y):\n",
    "    return np.mean((y-yhat)**2)\n",
    "\n",
    "rmse(yhat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGDregressor give the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# SGDRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(X, y)\n",
    "\n",
    "newx = np.linspace(min(X), max(X), 100)\n",
    "# and predict with those\n",
    "yhat = sgd.predict(newx)\n",
    "\n",
    "plt.scatter(X, y, label = 'data')\n",
    "plt.plot(newx, yhat, label = 'model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try how this works, if we scale the data.\n",
    "If we do that, we are no longer able to comfortably visualise the data. \n",
    "A trick is to simply plot actual vs prediction. It should be close to the diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X5, y5 = make_regression(n_samples=100, n_features=5, noise=25, random_state=42)\n",
    "\n",
    "sgd.fit(X5, y5)\n",
    "yhat = sgd.predict(X5)\n",
    "\n",
    "\n",
    "xmin = np.min([yhat, y5])\n",
    "xmax = np.max([yhat, y5])\n",
    "plt.scatter(y5, yhat)\n",
    "plt.plot([xmin, xmax], [xmin, xmax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also implement mini-batch with this.\n",
    "It's not really necessary with this small sample, but just to show how it is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 100 observations\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create 5 groups, and split it up into 5 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(x, columns=['x'])\n",
    "groups = np.random.randint(0, 5, size=n)\n",
    "data['groups'] = groups\n",
    "data['y'] = y\n",
    "batches = [x for _, x in data.groupby('groups')]\n",
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can train partial on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor()\n",
    "\n",
    "for batch in batches:\n",
    "    x_ = batch['x'].values.reshape(-1, 1)\n",
    "    y_ = batch['y'].values\n",
    "    sgd.partial_fit(x_, y_)\n",
    "\n",
    "yhat = linreg.predict(newx)\n",
    "\n",
    "plt.scatter(X, y, label = 'data')\n",
    "plt.plot(newx, yhat, label = 'model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different apporach is with probabilistic programming. \n",
    "The GLM function allows us to specify a formula. \"y ~ 1 +  x\" means: the basic formula is that we have an $y$ as a target value, and on the other hand a bias and an $x$. Try to find values for the bias and the $x$, such that it predicts the $y$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "with pm.Model() as model:\n",
    "    a = pm.Normal('a')\n",
    "    b = pm.Normal('b')\n",
    "    noise = pm.Normal('noise')\n",
    "\n",
    "    predict = a * x + b + noise\n",
    "    yhat = pm.Normal(\"y\", mu=predict, observed=y)\n",
    "\n",
    "    result = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.plot_trace(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = {}\n",
    "for key in result.posterior.data_vars.keys():\n",
    "    estimates[key] = result.posterior[key].mean().values\n",
    "estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An advantage of this approach is, that we get confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non linear data\n",
    "\n",
    "So, what if we have non-linear data? let's create some for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create 100 observations\n",
    "n = 100\n",
    "\n",
    "# this way, our X will go over the range [-3, 3]\n",
    "x = 6 * np.random.rand(n) - 3\n",
    "# let's use a quadratic equation\n",
    "a = 1.5\n",
    "b = 0.9\n",
    "c = 2.3\n",
    "noise = np.random.normal(size=n)\n",
    "\n",
    "y = c + b * x + a * np.power(x, 2) + noise\n",
    "data = {\"x\": x, \"y\": y}\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And try linear regression like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "X = x.reshape(-1, 1)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# make evenly spaced values over the input range\n",
    "newx = np.linspace(min(X), max(X), 100)\n",
    "# and predict with those\n",
    "yhat = linreg.predict(newx)\n",
    "\n",
    "plt.scatter(X, y, label = 'data')\n",
    "plt.plot(newx, yhat, label = 'model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well, we could have expected this... The model can only produce lines, so this will never work.\n",
    "\n",
    "Let's implement the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "poly = PolynomialFeatures(degree = 3, include_bias=True)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we expanded the amount of features we have. Because we have only one feature (x) to begin with, this does not explode. But if we would have had more features, using a high degree will very quickly blow up. Test this out for yourself with some artificial data. Change the shape to (100, 15) and first try to predict the shape of the outcome, then see what actually happens to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.random.rand(100, 5)\n",
    "p_big = PolynomialFeatures(degree = 3, include_bias=True)\n",
    "p_big.fit_transform(test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_poly, y)\n",
    "\n",
    "newx = np.linspace(min(X), max(X), 100)\n",
    "X_new_poly = poly.fit_transform(newx)\n",
    "yhat = linreg.predict(X_new_poly)\n",
    "\n",
    "plt.scatter(X, y, label = 'actual')\n",
    "plt.plot(newx, yhat, label = 'model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! Let's check the coefficients. Do you see how it actually fits the original weights `a` and `b` we used to create this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(linreg.coef_)), linreg.coef_)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn import model\n",
    "mymodel = model(parameters)\n",
    "mymodel.fit(X, y)\n",
    "yhat = mymodel.predict(newx)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the score function. This give a value between 0 and 1, the $R^2$ value. It is a slight modification of the RMSE and basically tells you how much of the variation of the data can be explained by your model.\n",
    "\n",
    "What counts as a high enough $R^2$ value depends on the context.\n",
    "\n",
    "Also note, we score on the train-set. This would be wrong with real data, you should use your test-set for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add a huge amount of features, just to see what happens.\n",
    "\n",
    "We will also put everything into a pipeline. This is especially usefull because we want to use the scaler *after* the polynomial features. Do you understand why? If not, discuss this with other students or ask the teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# set the degree to 50 (change it, to see what happens)\n",
    "degree = 50\n",
    "\n",
    "polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "std_scaler = StandardScaler()\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            (\"lin_reg\", lin_reg),\n",
    "        ])\n",
    "\n",
    "polynomial_regression.fit(X, y)\n",
    "\n",
    "newx = np.linspace(min(X), max(X)+1, 100)\n",
    "yhat = polynomial_regression.predict(newx)\n",
    "\n",
    "plt.scatter(X, y, label = 'actual')\n",
    "plt.plot(newx, yhat, label = 'model')\n",
    "plt.legend()\n",
    "plt.axis([-3.2, 3.2, 0, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what is happening? Well, we might have expected something like this. This is obviously overfitting, our model is much to complex. Now, let's try to add in some regularization again.\n",
    "\n",
    "First \"l1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_l1 = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            ('sgd', SGDRegressor(penalty='l1', alpha=5))\n",
    "        ])\n",
    "\n",
    "poly_l1.fit(X, y)\n",
    "\n",
    "\n",
    "newx = np.linspace(min(X), max(X), 100)\n",
    "yhat_l1 = poly_l1.predict(newx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_l2 = Pipeline([\n",
    "            (\"poly_features\", polybig_features),\n",
    "            (\"std_scaler\", std_scaler),\n",
    "            ('sgd', SGDRegressor(penalty='l2', alpha=100))\n",
    "        ])\n",
    "\n",
    "poly_l2.fit(X, y)\n",
    "yhat_l2 = poly_l2.predict(newx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y, label = 'actual')\n",
    "plt.plot(newx, yhat_l1, label = 'l1')\n",
    "plt.plot(newx, yhat_l2, label = 'l2')\n",
    "plt.legend()\n",
    "plt.axis([-3.2, 3.2, 0, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also try elasticnet. To do this, we will need to both tune the l1_ratio and the alpha values.\n",
    "\n",
    "It is much easier to do this in a parameter grid. Note how we use the double underscores `__` to connect the name of the function in the pipeline to the name of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [10**i for i in range(-3, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "     ('sgd', SGDRegressor(penalty='elasticnet'))\n",
    "])\n",
    "\n",
    "l1_ratio = [0, 0.01, 0.05, .1, .5, .7, .9, .95, .99, 1]\n",
    "alphaList = [10**i for i in range(-3, 2)]\n",
    "\n",
    "param_grid = {'sgd__l1_ratio' : l1_ratio,\n",
    "                'sgd__alpha' : alphaList}\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "gridsearch.fit(X, y)\n",
    "\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model picks the l1_ratio. \n",
    "\n",
    "To be clear: this is a very unrealistic case. We first set the amount of parameters to 50, which is way too high. And then we try to regulate this again. \n",
    "\n",
    "This is like hitting the gas and the brake at the same time, so the model we will produce isn't optimal for prediction. However, this shows you how to do something like this.\n",
    "\n",
    "Let's try a Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR with kernel\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(C=1, gamma=0.1)\n",
    "svr.fit(X, y)\n",
    "yhat = svr.predict(newx)\n",
    "plt.scatter(X, y, label = 'actual')\n",
    "plt.plot(newx, yhat, label = 'model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM has a natural protection against overfitting, because of the way it tries to find weights that have a big safety margin. The epsilon parameter specifies a tube in which no penalty is assigned with points predicted within a distance epsilon from the actual value.\n",
    "\n",
    "The gamma value is used to distort the space with the kernel. try to set it much higher (eg 10) to see what happens.\n",
    "\n",
    "# Probabilistic programming\n",
    "Next, we can specify a GLM for pymc3. Because our data consists of an `x` and `y` value, we can use those to specify a formula that contains a power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baysian regression\n",
    "with pm.Model() as model:\n",
    "    a = pm.Normal('a')\n",
    "    b = pm.Normal('b')\n",
    "    c = pm.Normal('c')\n",
    "\n",
    "    predict = a * x**2 + b * x + c\n",
    "    yhat = pm.Normal(\"y\", mu=predict, observed=y)\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we get close to the value. Downsides of this approach are speed and the need to specify a model. For that, you get a confidence interval in return and the flexibility to specify a model, based on your domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = {}\n",
    "for key in trace.posterior.data_vars.keys():\n",
    "    estimates[key] = trace.posterior[key].mean().values\n",
    "estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, let's try a random forest regressor. We will tune the depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "param_grid = dict(max_depth = [*range(2, 10, 2)])\n",
    "\n",
    "gridsearch = GridSearchCV(rfr, param_grid=param_grid, cv=3)\n",
    "gridsearch.fit(X, y)\n",
    "gridsearch.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = gridsearch.predict(newx)\n",
    "plt.scatter(X, y, label = 'actual')\n",
    "plt.plot(newx, yhat, label = 'randomforest')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ca2ed108e0829ad954ac36f354f1cc4b518ca95c97ddb3ce5ba4e3a95bf1dea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
